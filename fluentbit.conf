[SERVICE]
    flush        1
    log_level    trace
    parsers_file parsers.conf
    parsers_file /home/vcap/deps/0/apt/etc/fluent-bit/parsers.conf
    plugins_File plugins.conf

[INPUT]
    name      dummy
    dummy     {"message":"A simple test message", "temp": "0.74", "extra": "false"}
    samples   1


# TODO:
#  - Refer to extracted env vars in the config below
# [OUTPUT]
#      Name s3
#      Match *
#      bucket your-bucket
#      region us-east-1
#      store_dir /home/ec2-user/buffer
#      total_file_size 50M
#      upload_timeout 10m

[INPUT]
    name tcp
    port ${PORT}
    format none

### Filters run in order of appearance ###
# Combine multiple (headers + body)of an HTTP POST request into a single record.
[FILTER]
    name multiline
    match tcp.*
    multiline.key_content log
    multiline.parser combine-http-post

# Initial pass at parsing the body of the request.
[FILTER]
    name parser
    match tcp.*
    key_name log
    parser post-with-syslog
    # reserve_data: Keep all other fields
    reserve_data Off
    # preserve_key: Keep the key_name field (message)
    preserve_key On

# Further filter of the already-extracted message
[FILTER]
    name parser
    match tcp.*
    key_name message
    parser extract-gauge
    # reserve_data: Keep all other fields
    reserve_data On
    # preserve_key: Keep the key_name field (message)
    preserve_key On

# And so on ... (multiple passes let us capture fields in varying order)
[FILTER]
    name parser
    match tcp.*
    key_name message
    parser extract-tags
    reserve_data On
    preserve_key On

# Process selected keys (tags, gauge) into stringified JSON
[FILTER]
    name lua
    match tcp.*
    script scripts/parse_tags_with_eq_pairs.lua
    call parse_tags_with_eq_pairs

# [OUTPUT]
#     Name newrelic
#     Match *
#     licenseKey ${NEW_RELIC_LICENSE_KEY}
#     endpoint ${NEW_RELIC_LOGS_ENDPOINT}

# Comment this out to test by watching the logshipper's logs:
[OUTPUT]
    name stdout
    match *
